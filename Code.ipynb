{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85ea6a28-029c-4031-9bd1-13ec722edca0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73efb926-8656-4420-b721-7165a1bce29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 原始数据量: 6936\n",
      "✅ 增强后数据量: 13872\n",
      "模型参数量: 11685122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages/torch/utils/tensorboard/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, '__version__') or LooseVersion(tensorboard.__version__) < LooseVersion('1.15'):\n",
      "/opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 13872\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1302\n",
      "/opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages/torch/cuda/nccl.py:51: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  if not isinstance(inputs, collections.Container) or isinstance(inputs, torch.Tensor):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1302' max='1302' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1302/1302 06:44, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.202900</td>\n",
       "      <td>0.286843</td>\n",
       "      <td>0.883900</td>\n",
       "      <td>0.884000</td>\n",
       "      <td>0.837100</td>\n",
       "      <td>0.936400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.125400</td>\n",
       "      <td>0.372762</td>\n",
       "      <td>0.877900</td>\n",
       "      <td>0.872100</td>\n",
       "      <td>0.863100</td>\n",
       "      <td>0.881400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.084700</td>\n",
       "      <td>0.604765</td>\n",
       "      <td>0.878900</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.882400</td>\n",
       "      <td>0.858100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.042600</td>\n",
       "      <td>0.675797</td>\n",
       "      <td>0.896900</td>\n",
       "      <td>0.887900</td>\n",
       "      <td>0.912800</td>\n",
       "      <td>0.864400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.011800</td>\n",
       "      <td>0.737080</td>\n",
       "      <td>0.881900</td>\n",
       "      <td>0.872000</td>\n",
       "      <td>0.893300</td>\n",
       "      <td>0.851700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.709217</td>\n",
       "      <td>0.885900</td>\n",
       "      <td>0.878500</td>\n",
       "      <td>0.884100</td>\n",
       "      <td>0.872900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 999\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./results/checkpoint-217\n",
      "Configuration saved in ./results/checkpoint-217/config.json\n",
      "Model weights saved in ./results/checkpoint-217/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 999\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./results/checkpoint-434\n",
      "Configuration saved in ./results/checkpoint-434/config.json\n",
      "Model weights saved in ./results/checkpoint-434/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 999\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./results/checkpoint-651\n",
      "Configuration saved in ./results/checkpoint-651/config.json\n",
      "Model weights saved in ./results/checkpoint-651/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-434] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 999\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./results/checkpoint-868\n",
      "Configuration saved in ./results/checkpoint-868/config.json\n",
      "Model weights saved in ./results/checkpoint-868/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-217] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 999\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./results/checkpoint-1085\n",
      "Configuration saved in ./results/checkpoint-1085/config.json\n",
      "Model weights saved in ./results/checkpoint-1085/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-651] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 999\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./results/checkpoint-1302\n",
      "Configuration saved in ./results/checkpoint-1302/config.json\n",
      "Model weights saved in ./results/checkpoint-1302/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1085] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-868 (score: 0.8879).\n",
      "Configuration saved in ./results/config.json\n",
      "Model weights saved in ./results/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/tokenizer_config.json\n",
      "Special tokens file saved in ./results/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 模型训练完成并已保存到 ./results/\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Original augmented learning code (muting all nltk.download outputs)\n",
    "\n",
    "import nltk\n",
    "# First download normally (run only once, quiet=True silent mode)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "# Then rewrite download function to empty, blocking any subsequent download prints\n",
    "nltk.download = lambda *args, **kwargs: True\n",
    "\n",
    "from transformers import AlbertTokenizer, AlbertForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import nlpaug.augmenter.word as naw\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# -------- Synonym Augmenter --------\n",
    "aug = naw.SynonymAug(aug_src='wordnet')\n",
    "def augment_text(text):\n",
    "    try:\n",
    "        return aug.augment(text)\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "# -------- Custom Dataset Class --------\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts     = dataframe['Text'].tolist()\n",
    "        self.labels    = dataframe['Label'].tolist()\n",
    "        self.max_length= max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key,val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "\n",
    "# -------- Load and Augment Data --------\n",
    "def load_and_augment(path):\n",
    "    df = pd.read_csv(path, sep='\\t')\n",
    "    if df.columns[0] != 'Id':\n",
    "        df.columns = ['Id', 'Text', 'Label']\n",
    "    df['Label'] = df['Label'].map({'INFORMATIVE': 1, 'UNINFORMATIVE': 0})\n",
    "    print(f\"✅ Original data size: {len(df)}\")\n",
    "\n",
    "    aug_df = df.copy()\n",
    "    aug_df['Text'] = aug_df['Text'].apply(augment_text)\n",
    "    full_df = pd.concat([df, aug_df]).sample(frac=1).reset_index(drop=True)\n",
    "    print(f\"✅ Augmented data size: {len(full_df)}\")\n",
    "    return full_df\n",
    "\n",
    "# -------- Evaluation Metrics --------\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds  = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': round(acc, 4),\n",
    "        'f1':       round(f1, 4),\n",
    "        'precision':round(precision, 4),\n",
    "        'recall':   round(recall, 4)\n",
    "    }\n",
    "\n",
    "# -------- Custom Trainer (with weighted loss) --------\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits  = outputs.logits\n",
    "        weight  = torch.tensor([0.7, 1.7]).to(logits.device)\n",
    "        loss_fct= nn.CrossEntropyLoss(weight=weight)\n",
    "        loss    = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# -------- Model and Tokenizer --------\n",
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "model     = AlbertForSequenceClassification.from_pretrained('albert-base-v2', num_labels=2)\n",
    "\n",
    "# -------- Data Loading --------\n",
    "train_df = load_and_augment('./WNUT-2020-Task-2-Dataset/train.tsv')\n",
    "valid_df = pd.read_csv('./WNUT-2020-Task-2-Dataset/valid.tsv', sep='\\t')\n",
    "valid_df.columns = ['Id', 'Text', 'Label']\n",
    "valid_df['Label'] = valid_df['Label'].map({'INFORMATIVE': 1, 'UNINFORMATIVE': 0})\n",
    "train_ds = TextDataset(train_df, tokenizer)\n",
    "valid_ds = TextDataset(valid_df, tokenizer)\n",
    "\n",
    "# -------- Training Arguments --------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=128,\n",
    "    num_train_epochs=6,\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    save_total_limit=2,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    warmup_steps=500,\n",
    ")\n",
    "\n",
    "# -------- Trainer & Training --------\n",
    "print(f\"Model parameter count: {sum(p.numel() for p in model.parameters())}\")\n",
    "trainer = WeightedLossTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=valid_ds,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()\n",
    "model.save_pretrained('./results/')\n",
    "tokenizer.save_pretrained('./results/')\n",
    "print(\"✅ Model training completed and saved to ./results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9e0d23f-5249-4566-9f1f-877b77969100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./results/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"albert-base-v2\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file ./results/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing AlbertForSequenceClassification.\n",
      "\n",
      "All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at ./results/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.\n",
      "Didn't find file ./results/added_tokens.json. We won't load it.\n",
      "Didn't find file ./results/tokenizer.json. We won't load it.\n",
      "loading file ./results/spiece.model\n",
      "loading file None\n",
      "loading file ./results/special_tokens_map.json\n",
      "loading file ./results/tokenizer_config.json\n",
      "loading file None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Accuracy on test set: 0.88900\n",
      "📊 Classification Report:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "UNINFORMATIVE     0.8690    0.9299    0.8984      1056\n",
      "  INFORMATIVE     0.9149    0.8432    0.8776       944\n",
      "\n",
      "     accuracy                         0.8890      2000\n",
      "    macro avg     0.8920    0.8866    0.8880      2000\n",
      " weighted avg     0.8907    0.8890    0.8886      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def evaluate_on_test(model_path='./results/', test_path='WNUT-2020-Task-2-Dataset/test.tsv'):\n",
    "    # Load model and tokenizer\n",
    "    model = AlbertForSequenceClassification.from_pretrained(model_path)\n",
    "    tokenizer = AlbertTokenizer.from_pretrained(model_path)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Load test data\n",
    "    df = pd.read_csv(test_path, sep='\\t', header=None)\n",
    "    df.columns = ['Id', 'Text', 'Label']\n",
    "    df['Label'] = df['Label'].map({'UNINFORMATIVE': 0, 'INFORMATIVE': 1})\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _, row in df.iterrows():\n",
    "            text = row['Text']\n",
    "            label = row['Label']\n",
    "            inputs = tokenizer(\n",
    "                text,\n",
    "                return_tensors='pt',\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=128\n",
    "            ).to(device)\n",
    "            outputs = model(**inputs)\n",
    "            pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "            y_pred.append(pred)\n",
    "            y_true.append(label)\n",
    "\n",
    "    # Print results\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, target_names=['UNINFORMATIVE', 'INFORMATIVE'], digits=4)\n",
    "\n",
    "    print(f\"\\n✅ Accuracy on test set: {acc:.5f}\")\n",
    "    print(\"📊 Classification Report:\\n\")\n",
    "    print(report)\n",
    "\n",
    "# Run the evaluation\n",
    "evaluate_on_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e8a16b-b0f9-4dba-a76d-fe6d04df3494",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 原始数据量: 6936\n",
      "✅ 增强后数据量: 13872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pseudo-labels: 100%|██████████| 375/375 [00:40<00:00,  9.18it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成 17 条高置信度伪标签（阈值 0.7）\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages/torch/utils/tensorboard/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, '__version__') or LooseVersion(tensorboard.__version__) < LooseVersion('1.15'):\n",
      "/opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 13889\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 3: Training with pseudo-labeled data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='629' max='1305' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 629/1305 02:46 < 02:59, 3.77 it/s, Epoch 1.44/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.184300</td>\n",
       "      <td>0.410443</td>\n",
       "      <td>0.869900</td>\n",
       "      <td>0.852900</td>\n",
       "      <td>0.915000</td>\n",
       "      <td>0.798700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./results/checkpoint-435\n",
      "Configuration saved in ./results/checkpoint-435/config.json\n",
      "Model weights saved in ./results/checkpoint-435/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-3105] due to args.save_total_limit\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# Hijack download function, block any nltk.download logs\n",
    "nltk.download = lambda *args, **kwargs: True\n",
    "\n",
    "from transformers import AlbertTokenizer, AlbertForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import nlpaug.augmenter.word as naw\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt  # For plotting\n",
    "\n",
    "# ----- Redefine training_args (consistent with Cell 1) -----\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=128,\n",
    "    num_train_epochs=6,\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    save_total_limit=2,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    warmup_steps=500,\n",
    ")\n",
    "\n",
    "# -------- Synonym Augmenter --------\n",
    "aug = naw.SynonymAug(aug_src='wordnet')\n",
    "def augment_text(text):\n",
    "    try:\n",
    "        return aug.augment(text)\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "# -------- Custom Dataset Class --------\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts     = dataframe['Text'].tolist()\n",
    "        self.labels    = dataframe['Label'].tolist()\n",
    "        self.max_length= max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key,val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "# -------- Load and Augment Data --------\n",
    "def load_and_augment(path):\n",
    "    df = pd.read_csv(path, sep='\\t')\n",
    "    if df.columns[0] != 'Id':\n",
    "        df.columns = ['Id', 'Text', 'Label']\n",
    "    df['Label'] = df['Label'].map({'INFORMATIVE': 1, 'UNINFORMATIVE': 0})\n",
    "    print(f\"✅ Original data size: {len(df)}\")\n",
    "\n",
    "    aug_df = df.copy()\n",
    "    aug_df['Text'] = aug_df['Text'].apply(augment_text)\n",
    "    full_df = pd.concat([df, aug_df]).sample(frac=1).reset_index(drop=True)\n",
    "    print(f\"✅ Augmented data size: {len(full_df)}\")\n",
    "    return full_df\n",
    "\n",
    "# -------- Evaluation Metrics --------\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds  = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': round(acc, 4),\n",
    "        'f1':       round(f1, 4),\n",
    "        'precision':round(precision, 4),\n",
    "        'recall':   round(recall, 4)\n",
    "    }\n",
    "\n",
    "# -------- Custom Trainer (with weighted loss) --------\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits  = outputs.logits\n",
    "        weight  = torch.tensor([0.7, 1.7]).to(logits.device)\n",
    "        loss_fct= nn.CrossEntropyLoss(weight=weight)\n",
    "        loss    = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# -------- Model and Tokenizer --------\n",
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "model     = AlbertForSequenceClassification.from_pretrained('albert-base-v2', num_labels=2)\n",
    "\n",
    "# -------- Data Loading --------\n",
    "train_df = load_and_augment('./WNUT-2020-Task-2-Dataset/train.tsv')\n",
    "valid_df = pd.read_csv('./WNUT-2020-Task-2-Dataset/valid.tsv', sep='\\t')\n",
    "valid_df.columns = ['Id', 'Text', 'Label']\n",
    "valid_df['Label'] = valid_df['Label'].map({'INFORMATIVE': 1, 'UNINFORMATIVE': 0})\n",
    "train_ds = TextDataset(train_df, tokenizer)\n",
    "valid_ds = TextDataset(valid_df, tokenizer)\n",
    "\n",
    "# -------- Load Unlabeled Data --------\n",
    "unlabeled_df = pd.read_csv(\n",
    "    './WNUT-2020-Task-2-Dataset/unlabeled_test_with_noise.tsv',\n",
    "    sep='\\t', names=['Id','Text'], header=0\n",
    ")\n",
    "unlabeled_df['Label'] = 0\n",
    "unlabeled_ds = TextDataset(unlabeled_df, tokenizer, max_length=128)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def generate_pseudo_labels_with_plot(model, dataset, device,\n",
    "                                     batch_size=32, threshold=0.7):\n",
    "    model.to(device).eval()\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    confidences, preds = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Generating pseudo-labels\"):\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k!='labels'}\n",
    "            logits = model(**inputs).logits\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            conf, pr = torch.max(probs, dim=-1)\n",
    "            confidences.extend(conf.cpu().tolist())\n",
    "            preds.extend(pr.cpu().tolist())\n",
    "\n",
    "    # Visualize confidence distribution\n",
    "    plt.figure()\n",
    "    plt.hist(confidences, bins=30)\n",
    "    plt.title('Confidence Score Distribution')\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "    # Filter high-confidence pseudo labels\n",
    "    indices = [i for i, c in enumerate(confidences) if c >= threshold]\n",
    "    print(f\"Generated {len(indices)} high-confidence pseudo labels (threshold {threshold})\")\n",
    "    return indices, [preds[i] for i in indices]\n",
    "\n",
    "# Phase 2: Generate and plot confidence distribution\n",
    "sel_idx, pseudo_lbls = generate_pseudo_labels_with_plot(model, unlabeled_ds, device, threshold=0.7)\n",
    "\n",
    "if sel_idx:\n",
    "    pseudo_df = unlabeled_df.iloc[sel_idx].copy()\n",
    "    pseudo_df['Label'] = pseudo_lbls\n",
    "    combined_df = pd.concat([train_df, pseudo_df], ignore_index=True)\n",
    "    combined_ds = TextDataset(combined_df, tokenizer, max_length=128)\n",
    "\n",
    "    # Adjust learning rate and epochs\n",
    "    training_args.num_train_epochs = 3\n",
    "    training_args.learning_rate    = 1e-5\n",
    "\n",
    "    pseudo_trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=combined_ds,\n",
    "        eval_dataset=valid_ds,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    print(\"\\nPhase 3: Training with pseudo-labeled data...\")\n",
    "    pseudo_trainer.train()\n",
    "    print(\"\\nFinal evaluation:\", pseudo_trainer.evaluate())\n",
    "\n",
    "    model.save_pretrained('./final_model/')\n",
    "    tokenizer.save_pretrained('./final_model/')\n",
    "    print(\"✅ Self-supervised learning completed and saved to ./final_model/\")\n",
    "else:\n",
    "    print(\"No high-confidence pseudo labels generated, skipping Phase 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1a9ab43-0283-488d-a31c-9b79703360d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded columns: Index(['Id', 'Text', 'Label'], dtype='object')\n",
      "Loaded columns: Index(['Id', 'Text', 'Label'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages/torch/utils/tensorboard/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, '__version__') or LooseVersion(tensorboard.__version__) < LooseVersion('1.15'):\n",
      "/opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 6936\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1302\n",
      "/opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages/torch/cuda/nccl.py:51: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  if not isinstance(inputs, collections.Container) or isinstance(inputs, torch.Tensor):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1302' max='1302' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1302/1302 04:49, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.361900</td>\n",
       "      <td>0.348907</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.861822</td>\n",
       "      <td>0.833663</td>\n",
       "      <td>0.891949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.202200</td>\n",
       "      <td>0.313908</td>\n",
       "      <td>0.874875</td>\n",
       "      <td>0.864572</td>\n",
       "      <td>0.884701</td>\n",
       "      <td>0.845339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.148900</td>\n",
       "      <td>0.357498</td>\n",
       "      <td>0.890891</td>\n",
       "      <td>0.887975</td>\n",
       "      <td>0.862275</td>\n",
       "      <td>0.915254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.093500</td>\n",
       "      <td>0.545838</td>\n",
       "      <td>0.889890</td>\n",
       "      <td>0.887295</td>\n",
       "      <td>0.859127</td>\n",
       "      <td>0.917373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.035800</td>\n",
       "      <td>0.571410</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.887265</td>\n",
       "      <td>0.874486</td>\n",
       "      <td>0.900424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.017800</td>\n",
       "      <td>0.613479</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.885212</td>\n",
       "      <td>0.864646</td>\n",
       "      <td>0.906780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./results/checkpoint-217\n",
      "Configuration saved in ./results/checkpoint-217/config.json\n",
      "Model weights saved in ./results/checkpoint-217/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-651] due to args.save_total_limit\n",
      "/opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./results/checkpoint-434\n",
      "Configuration saved in ./results/checkpoint-434/config.json\n",
      "Model weights saved in ./results/checkpoint-434/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-285] due to args.save_total_limit\n",
      "/opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./results/checkpoint-651\n",
      "Configuration saved in ./results/checkpoint-651/config.json\n",
      "Model weights saved in ./results/checkpoint-651/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-217] due to args.save_total_limit\n",
      "/opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./results/checkpoint-868\n",
      "Configuration saved in ./results/checkpoint-868/config.json\n",
      "Model weights saved in ./results/checkpoint-868/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-434] due to args.save_total_limit\n",
      "/opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./results/checkpoint-1085\n",
      "Configuration saved in ./results/checkpoint-1085/config.json\n",
      "Model weights saved in ./results/checkpoint-1085/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-868] due to args.save_total_limit\n",
      "/opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./results/checkpoint-1302\n",
      "Configuration saved in ./results/checkpoint-1302/config.json\n",
      "Model weights saved in ./results/checkpoint-1302/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1085] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-651 (score: 0.8879753340184995).\n",
      "Configuration saved in ./results/config.json\n",
      "Model weights saved in ./results/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/tokenizer_config.json\n",
      "Special tokens file saved in ./results/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./results/tokenizer_config.json',\n",
       " './results/special_tokens_map.json',\n",
       " './results/spiece.model',\n",
       " './results/added_tokens.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AlbertTokenizer, AlbertForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = dataframe['Text'].tolist()\n",
    "        self.labels = dataframe['Label'].tolist()\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "# Load data\n",
    "def load_data(path):\n",
    "    df = pd.read_csv(path, sep='\\t')\n",
    "    if df.columns[0] != 'Id':\n",
    "        df.columns = ['Id', 'Text', 'Label']\n",
    "    print(f\"Loaded columns: {df.columns}\")\n",
    "    df['Label'] = df['Label'].map({'INFORMATIVE': 1, 'UNINFORMATIVE': 0})\n",
    "    return df\n",
    "\n",
    "# Evaluation function\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "model = AlbertForSequenceClassification.from_pretrained('albert-base-v2', num_labels=2)\n",
    "\n",
    "# Load datasets\n",
    "train_df = load_data('./WNUT-2020-Task-2-Dataset/train.tsv')\n",
    "valid_df = load_data('./WNUT-2020-Task-2-Dataset/valid.tsv')\n",
    "train_ds = TextDataset(train_df, tokenizer)\n",
    "valid_ds = TextDataset(valid_df, tokenizer)\n",
    "\n",
    "# Training arguments - Add regularization and early stopping\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=6,           # Reduce number of epochs\n",
    "    weight_decay=0.01,            # Add weight decay\n",
    "    learning_rate=2e-5,           # Set a smaller learning rate\n",
    "    save_total_limit=2,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"epoch\",   # Evaluate once per epoch\n",
    "    save_strategy=\"epoch\",         # Save once per epoch\n",
    "    load_best_model_at_end=True,   # Load the best model\n",
    "    metric_for_best_model=\"f1\",    # Use F1 as the metric for selecting the best model\n",
    "    warmup_steps=500,              # Warmup steps\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=valid_ds,\n",
    "    compute_metrics=compute_metrics,  # Add evaluation metrics\n",
    ")\n",
    "\n",
    "# Training\n",
    "trainer.train()\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained('./results/')\n",
    "tokenizer.save_pretrained('./results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495908ac-4e84-4ca4-bf08-8b5caa17a872",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Gen pseudo: 100%|██████████| 375/375 [00:39<00:00,  9.42it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages/torch/utils/tensorboard/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, '__version__') or LooseVersion(tensorboard.__version__) < LooseVersion('1.15'):\n",
      "/opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 25870\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Phase 3: Training on augmented supervised + pseudo data …\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4854' max='4854' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4854/4854 21:22, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.437600</td>\n",
       "      <td>0.743279</td>\n",
       "      <td>0.778800</td>\n",
       "      <td>0.718500</td>\n",
       "      <td>0.901000</td>\n",
       "      <td>0.597500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.302200</td>\n",
       "      <td>1.009234</td>\n",
       "      <td>0.746700</td>\n",
       "      <td>0.649100</td>\n",
       "      <td>0.939800</td>\n",
       "      <td>0.495800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.155100</td>\n",
       "      <td>1.834762</td>\n",
       "      <td>0.726700</td>\n",
       "      <td>0.604900</td>\n",
       "      <td>0.954300</td>\n",
       "      <td>0.442800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.093200</td>\n",
       "      <td>2.190710</td>\n",
       "      <td>0.747700</td>\n",
       "      <td>0.665800</td>\n",
       "      <td>0.890100</td>\n",
       "      <td>0.531800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.026300</td>\n",
       "      <td>2.876311</td>\n",
       "      <td>0.750800</td>\n",
       "      <td>0.664900</td>\n",
       "      <td>0.911400</td>\n",
       "      <td>0.523300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>2.999413</td>\n",
       "      <td>0.750800</td>\n",
       "      <td>0.666700</td>\n",
       "      <td>0.905500</td>\n",
       "      <td>0.527500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./results/checkpoint-809\n",
      "Configuration saved in ./results/checkpoint-809/config.json\n",
      "Model weights saved in ./results/checkpoint-809/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-870] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./results/checkpoint-1618\n",
      "Configuration saved in ./results/checkpoint-1618/config.json\n",
      "Model weights saved in ./results/checkpoint-1618/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1305] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./results/checkpoint-2427\n",
      "Configuration saved in ./results/checkpoint-2427/config.json\n",
      "Model weights saved in ./results/checkpoint-2427/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1618] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./results/checkpoint-3236\n",
      "Configuration saved in ./results/checkpoint-3236/config.json\n",
      "Model weights saved in ./results/checkpoint-3236/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-2427] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./results/checkpoint-4045\n",
      "Configuration saved in ./results/checkpoint-4045/config.json\n",
      "Model weights saved in ./results/checkpoint-4045/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-3236] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 999\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./results/checkpoint-4854\n",
      "Configuration saved in ./results/checkpoint-4854/config.json\n",
      "Model weights saved in ./results/checkpoint-4854/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-4045] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-809 (score: 0.7185).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 999\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./final_model/config.json\n",
      "Model weights saved in ./final_model/pytorch_model.bin\n",
      "tokenizer config file saved in ./final_model/tokenizer_config.json\n",
      "Special tokens file saved in ./final_model/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Final evaluation: {'eval_loss': 0.7432789206504822, 'eval_accuracy': 0.7788, 'eval_f1': 0.7185, 'eval_precision': 0.901, 'eval_recall': 0.5975, 'eval_runtime': 3.215, 'eval_samples_per_second': 310.732, 'eval_steps_per_second': 2.488, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./final_model/tokenizer_config.json',\n",
       " './final_model/special_tokens_map.json',\n",
       " './final_model/spiece.model',\n",
       " './final_model/added_tokens.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 2: Self-Training with Top 50% Pseudo-Labels + Augmentation\n",
    "\n",
    "import os\n",
    "# Limit threads to avoid threading/memory errors\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import nltk\n",
    "# Block any nltk.download logs\n",
    "nltk.download = lambda *args, **kwargs: True\n",
    "\n",
    "from transformers import AlbertTokenizer, AlbertForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import nlpaug.augmenter.word as naw\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Redefine training_args (consistent with Cell 1)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=128,\n",
    "    num_train_epochs=6,\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    save_total_limit=2,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    warmup_steps=500,\n",
    ")\n",
    "# -------- Evaluation Metrics --------\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds  = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': round(acc, 4),\n",
    "        'f1':       round(f1, 4),\n",
    "        'precision':round(precision, 4),\n",
    "        'recall':   round(recall, 4)\n",
    "    }\n",
    "# Synonym augmenter\n",
    "aug = naw.SynonymAug(aug_src='wordnet')\n",
    "def augment_text(text):\n",
    "    try:\n",
    "        return aug.augment(text)\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "# Basic Dataset\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts     = dataframe['Text'].tolist()\n",
    "        self.labels    = dataframe['Label'].tolist()\n",
    "        self.max_length= max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "# Load & augment labeled data\n",
    "def load_and_augment(path):\n",
    "    df = pd.read_csv(path, sep='\\t', header=0)\n",
    "    if df.columns[0] != 'Id':\n",
    "        df.columns = ['Id','Text','Label']\n",
    "    df['Label'] = df['Label'].map({'INFORMATIVE':1,'UNINFORMATIVE':0})\n",
    "    aug_df = df.copy()\n",
    "    aug_df['Text'] = aug_df['Text'].apply(augment_text)\n",
    "    return pd.concat([df, aug_df]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train_df = load_and_augment('./WNUT-2020-Task-2-Dataset/train.tsv')\n",
    "valid_df = pd.read_csv(\n",
    "    './WNUT-2020-Task-2-Dataset/valid.tsv', \n",
    "    sep='\\t', header=0, names=['Id','Text','Label']\n",
    ")\n",
    "valid_df['Label'] = valid_df['Label'].map({'INFORMATIVE':1,'UNINFORMATIVE':0})\n",
    "\n",
    "# Load unlabeled data\n",
    "unlab_df = pd.read_csv(\n",
    "    './WNUT-2020-Task-2-Dataset/unlabeled_test_with_noise.tsv',\n",
    "    sep='\\t', header=0, names=['Id','Text']\n",
    ")\n",
    "unlab_df['Label'] = 0  # Placeholder\n",
    "\n",
    "# Initialize model & tokenizer\n",
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "model     = AlbertForSequenceClassification.from_pretrained('albert-base-v2', num_labels=2)\n",
    "device    = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Generate pseudo labels & collect confidence\n",
    "model.to(device).eval()\n",
    "loader = DataLoader(TextDataset(unlab_df, tokenizer), batch_size=32, shuffle=False)\n",
    "confs, preds = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(loader, desc=\"Gen pseudo\"):\n",
    "        inp = {k: v.to(device) for k, v in batch.items() if k!='labels'}\n",
    "        logits = model(**inp).logits\n",
    "        prob   = F.softmax(logits, dim=-1)\n",
    "        c, p   = torch.max(prob, dim=1)\n",
    "        confs.extend(c.cpu().tolist())\n",
    "        preds.extend(p.cpu().tolist())\n",
    "\n",
    "# Fill results back to DataFrame\n",
    "unlab_df['Confidence'] = confs\n",
    "unlab_df['Pred']       = preds\n",
    "\n",
    "# Visualize overall confidence distribution\n",
    "plt.hist(confs, bins=30)\n",
    "plt.title('Pseudo-Label Confidence Distribution')\n",
    "plt.xlabel('Confidence')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Select Top 50% pseudo labels\n",
    "k       = len(unlab_df) // 2\n",
    "top_df  = unlab_df.nlargest(k, 'Confidence').copy()\n",
    "top_df['Label'] = top_df['Pred']\n",
    "\n",
    "# Augment Top 50% pseudo labels\n",
    "aug_pseudo = top_df.copy()\n",
    "aug_pseudo['Text'] = aug_pseudo['Text'].apply(augment_text)\n",
    "\n",
    "# Merge pseudo labels before and after augmentation\n",
    "pseudo_combined = pd.concat([\n",
    "    top_df[['Text','Label']], \n",
    "    aug_pseudo[['Text','Label']]\n",
    "]).reset_index(drop=True)\n",
    "\n",
    "# Build final training set\n",
    "final_df = pd.concat([\n",
    "    train_df[['Text','Label']],  # Original labels + augmentation\n",
    "    pseudo_combined             # Pseudo labels + augmentation\n",
    "]).reset_index(drop=True)\n",
    "\n",
    "final_ds = TextDataset(final_df, tokenizer)\n",
    "\n",
    "# Final weighted training (using WeightedLossTrainer)\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels  = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits  = outputs.logits\n",
    "        weight  = torch.tensor([0.7,1.7]).to(device)\n",
    "        loss_fn = nn.CrossEntropyLoss(weight=weight)\n",
    "        loss    = loss_fn(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "trainer_final = WeightedLossTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=final_ds,\n",
    "    eval_dataset=TextDataset(valid_df, tokenizer),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"\\n▶ Phase 3: Training on augmented supervised + pseudo data …\")\n",
    "trainer_final.train()\n",
    "print(\"\\n▶ Final evaluation:\", trainer_final.evaluate())\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained('./final_model/')\n",
    "tokenizer.save_pretrained('./final_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a39089-9135-4438-a9bb-8d70d49bce2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./final_model/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"albert-base-v2\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file ./final_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing AlbertForSequenceClassification.\n",
      "\n",
      "All the weights of AlbertForSequenceClassification were initialized from the model checkpoint at ./final_model/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForSequenceClassification for predictions without further training.\n",
      "Didn't find file ./final_model/added_tokens.json. We won't load it.\n",
      "Didn't find file ./final_model/tokenizer.json. We won't load it.\n",
      "loading file ./final_model/spiece.model\n",
      "loading file None\n",
      "loading file ./final_model/special_tokens_map.json\n",
      "loading file ./final_model/tokenizer_config.json\n",
      "loading file None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Accuracy on test set: 0.73250\n",
      "📊 Classification Report:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "UNINFORMATIVE     0.6731    0.9593    0.7911      1056\n",
      "  INFORMATIVE     0.9131    0.4788    0.6282       944\n",
      "\n",
      "     accuracy                         0.7325      2000\n",
      "    macro avg     0.7931    0.7190    0.7097      2000\n",
      " weighted avg     0.7864    0.7325    0.7142      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def evaluate_on_test(model_path='./final_model/', test_path='WNUT-2020-Task-2-Dataset/test.tsv'):\n",
    "    # Load model and tokenizer\n",
    "    model = AlbertForSequenceClassification.from_pretrained(model_path)\n",
    "    tokenizer = AlbertTokenizer.from_pretrained(model_path)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Load test data\n",
    "    df = pd.read_csv(test_path, sep='\\t', header=None)\n",
    "    df.columns = ['Id', 'Text', 'Label']\n",
    "    df['Label'] = df['Label'].map({'UNINFORMATIVE': 0, 'INFORMATIVE': 1})\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _, row in df.iterrows():\n",
    "            text = row['Text']\n",
    "            label = row['Label']\n",
    "            inputs = tokenizer(\n",
    "                text,\n",
    "                return_tensors='pt',\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=128\n",
    "            ).to(device)\n",
    "            outputs = model(**inputs)\n",
    "            pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "            y_pred.append(pred)\n",
    "            y_true.append(label)\n",
    "\n",
    "    # Print results\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, target_names=['UNINFORMATIVE', 'INFORMATIVE'], digits=4)\n",
    "\n",
    "    print(f\"\\n✅ Accuracy on test set: {acc:.5f}\")\n",
    "    print(\"📊 Classification Report:\\n\")\n",
    "    print(report)\n",
    "\n",
    "# Run the evaluation\n",
    "evaluate_on_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:opence-v1.5.1]",
   "language": "python",
   "name": "conda-env-opence-v1.5.1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
